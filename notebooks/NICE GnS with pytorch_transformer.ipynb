{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "Collecting sacremoses (from pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/24/0b86f494d3a5c7531f6d0c77d39fd8f9d42e651244505d3d737e31db9a4d/sacremoses-0.0.33.tar.gz (802kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (2.20.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.15.4)\n",
      "Collecting sentencepiece (from pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (4.32.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (2018.1.10)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.9.213)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (1.11.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (6.7)\n",
      "Collecting joblib (from sacremoses->pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.213 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (1.12.213)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.213->boto3->pytorch-transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.213->boto3->pytorch-transformers) (2.7.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses: started\n",
      "  Running setup.py bdist_wheel for sacremoses: finished with status 'done'\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/70/87/56/e40575cca30d12fee8875d523b8878b7aba866a9f03b2fd983\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: joblib, sacremoses, sentencepiece, pytorch-transformers\n",
      "Successfully installed joblib-0.13.2 pytorch-transformers-1.2.0 sacremoses-0.0.33 sentencepiece-0.1.83\n",
      "Collecting tensorboardX (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (3.5.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorboardX->-r requirements.txt (line 1)) (39.1.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.8\n",
      "Collecting protobuf\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/f4/a27952733796330cd17c17ea1f974459f5fefbbad119c0f296a6d807fec3/protobuf-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Requirement not upgraded as not directly required: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf) (39.1.0)\n",
      "Requirement not upgraded as not directly required: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf) (1.11.0)\n",
      "Installing collected packages: protobuf\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed protobuf-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pytorch-transformers\n",
    "\n",
    "cd /home/ec2-user/SageMaker/pytorch-transformers/examples\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "#Fixing error with from tensorboardX import SummaryWriter\n",
    "#TypeError: __new__() got an unexpected keyword argument 'serialized_options'\n",
    "\n",
    "source activate pytorch_p36\n",
    "pip install protobuf -U\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Generate Data suitable with run_glue example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>combusting preparations [chemical additives to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10002</td>\n",
       "      <td>adhesives for industrial purposes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10003</td>\n",
       "      <td>salt for preserving, other than for foodstuffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10004</td>\n",
       "      <td>auxiliary fluids for use with abrasives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>vulcanization accelerators</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Classification                                                 EN\n",
       "0      1           10001  combusting preparations [chemical additives to...\n",
       "1      1           10002                  adhesives for industrial purposes\n",
       "2      1           10003     salt for preserving, other than for foodstuffs\n",
       "3      1           10004            auxiliary fluids for use with abrasives\n",
       "4      1           10005                         vulcanization accelerators"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'ipa-cognitive-futures'\n",
    "object_name = 'cf_datasets/TM/raw/nc020-a04_ibli.csv'\n",
    "file_name = 'nc020-a04_ibli.csv'\n",
    "semantic_column = 'EN'\n",
    "\n",
    "s3.download_file(bucket_name, object_name, file_name)\n",
    "\n",
    "import pandas as pd\n",
    "nice_df = pd.read_csv(file_name, header=0,encoding='latin1')\n",
    "\n",
    "#correct columns\n",
    "retrieved_columns = ['Class','Classification','EN', 'FR']\n",
    "columns = retrieved_columns + list(nice_df.columns)[4:]\n",
    "\n",
    "#obtain EN GS\n",
    "nice_df.columns = columns\n",
    "nice_df = nice_df[retrieved_columns]\n",
    "del nice_df['FR']\n",
    "nice_df.dropna(inplace=True)\n",
    "nice_df['Class'] = nice_df['Class'].astype(int)\n",
    "nice_df['Classification'] = nice_df['Classification'].astype(int)\n",
    "nice_df.reset_index(inplace=True,drop=True)\n",
    "nice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train, dev = train_test_split(nice_df, train_size=0.8, random_state=7, stratify=list(nice_df['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>752</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>613</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 752  752\n",
       "2                 132  132\n",
       "3                 272  272\n",
       "4                 110  110\n",
       "5                 525  525\n",
       "6                 506  506\n",
       "7                 613  613\n",
       "8                 300  300\n",
       "9                 832  832\n",
       "10                298  298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice_df.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>405</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>666</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 602  602\n",
       "2                 106  106\n",
       "3                 218  218\n",
       "4                  88   88\n",
       "5                 420  420\n",
       "6                 405  405\n",
       "7                 490  490\n",
       "8                 240  240\n",
       "9                 666  666\n",
       "10                238  238"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 150  150\n",
       "2                  26   26\n",
       "3                  54   54\n",
       "4                  22   22\n",
       "5                 105  105\n",
       "6                 101  101\n",
       "7                 123  123\n",
       "8                  60   60\n",
       "9                 166  166\n",
       "10                 60   60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns'\n",
    "#train.to_csv(dataset_folder + '/train.tsv', sep='\\t', index=False, header=False)\n",
    "#dev.to_csv(dataset_folder + '/dev.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Classification</th>\n",
       "      <th>En</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>200235</td>\n",
       "      <td>stools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>30082</td>\n",
       "      <td>diamantine [abrasive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>250095</td>\n",
       "      <td>cuffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>220071</td>\n",
       "      <td>tents*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>110135</td>\n",
       "      <td>purification installations for sewage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Classification                                     En\n",
       "0     20          200235                                 stools\n",
       "1      3           30082                  diamantine [abrasive]\n",
       "2     25          250095                                  cuffs\n",
       "3     22          220071                                 tents*\n",
       "4     11          110135  purification installations for sewage"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(dataset_folder + '/train.tsv', sep='\\t',  header=None)\n",
    "train.columns = ['Class','Classification','En']\n",
    "dev = pd.read_csv(dataset_folder + '/dev.tsv', sep='\\t',  header=None)\n",
    "dev.columns = ['Class','Classification','En']\n",
    "dev.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Glue Training on GnS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/19/2019 14:41:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "09/19/2019 14:41:05 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "09/19/2019 14:41:05 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 14:41:06 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ec2-user/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "09/19/2019 14:41:07 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "09/19/2019 14:41:11 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "09/19/2019 14:41:11 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=15, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=30.0, output_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=512, per_gpu_train_batch_size=512, save_steps=50, seed=42, server_ip='', server_port='', task_name='gns', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   Loading features from cached file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns/cached_train_bert-base-uncased_15_gns\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   ***** Running training *****\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Num examples = 8144\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Num Epochs = 30\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Instantaneous batch size per GPU = 512\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Total optimization steps = 480\n",
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:38,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:35,  2.52s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:26,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.44s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:21,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:21<00:17,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:26<00:12,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:31<00:07,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.43s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:38<00:00,  2.38s/it]\u001b[A\r",
      "Epoch:   3%|▎         | 1/30 [00:38<18:43, 38.74s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.44s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.44s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.44s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:26,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:26<00:12,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:31<00:07,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.40s/it]\u001b[A\r",
      "Epoch:   7%|▋         | 2/30 [01:17<18:07, 38.83s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  10%|█         | 3/30 [01:56<17:31, 38.95s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A09/19/2019 14:43:19 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-50\n",
      "\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:08<00:51,  3.65s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:11<00:43,  3.32s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:13<00:36,  3.06s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:16<00:31,  2.88s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:27,  2.75s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:23,  2.66s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:23<00:20,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:17,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:15,  2.53s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:12,  2.51s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:09,  2.50s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:35<00:07,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:40<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:43<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  13%|█▎        | 4/30 [02:40<17:25, 40.21s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  17%|█▋        | 5/30 [03:19<16:38, 39.93s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.42s/it]\u001b[A\r",
      "Epoch:  20%|██        | 6/30 [03:58<15:54, 39.76s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A09/19/2019 14:45:26 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-100\n",
      "\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:13<00:43,  3.65s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:16<00:36,  3.31s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:30,  3.06s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:25,  2.88s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:23<00:22,  2.75s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:18,  2.67s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:15,  2.61s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:12,  2.57s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:10,  2.54s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.51s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.50s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.49s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:43<00:00,  2.43s/it]\u001b[A\r",
      "Epoch:  23%|██▎       | 7/30 [04:42<15:38, 40.81s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  27%|██▋       | 8/30 [05:21<14:47, 40.36s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.50s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.49s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.49s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  30%|███       | 9/30 [06:00<14:01, 40.06s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A09/19/2019 14:47:33 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-150\n",
      "\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:36,  3.65s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:29,  3.31s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:23<00:24,  3.05s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:20,  2.88s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:16,  2.76s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:13,  2.67s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:10,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.53s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:40<00:02,  2.51s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:43<00:00,  2.44s/it]\u001b[A\r",
      "Epoch:  33%|███▎      | 10/30 [06:43<13:40, 41.02s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  37%|███▋      | 11/30 [07:23<12:49, 40.50s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  40%|████      | 12/30 [08:02<12:02, 40.14s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A09/19/2019 14:49:39 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-200\n",
      "\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:23<00:29,  3.64s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:23,  3.31s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:18,  3.06s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:14,  2.88s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:11,  2.75s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.66s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:43<00:00,  2.48s/it]\u001b[A\r",
      "Epoch:  43%|████▎     | 13/30 [08:45<11:38, 41.08s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  47%|████▋     | 14/30 [09:25<10:48, 40.56s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.45s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  50%|█████     | 15/30 [10:04<10:02, 40.19s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A09/19/2019 14:51:46 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-250\n",
      "\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:21,  3.65s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:16,  3.32s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:12,  3.06s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:08,  2.88s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.76s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.67s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:43<00:00,  2.55s/it]\u001b[A\r",
      "Epoch:  53%|█████▎    | 16/30 [10:47<09:35, 41.14s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  57%|█████▋    | 17/30 [11:27<08:47, 40.60s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  60%|██████    | 18/30 [12:06<08:02, 40.20s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.48s/it]\u001b[A09/19/2019 14:53:50 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-300\n",
      "\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:30<00:10,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:35<00:05,  2.54s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.52s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.44s/it]\u001b[A\r",
      "Epoch:  63%|██████▎   | 19/30 [12:46<07:20, 40.08s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  67%|██████▋   | 20/30 [13:25<06:38, 39.84s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.42s/it]\u001b[A\r",
      "Epoch:  70%|███████   | 21/30 [14:04<05:57, 39.69s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A09/19/2019 14:55:53 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-350\n",
      "\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:35<00:05,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.56s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.47s/it]\u001b[A\r",
      "Epoch:  73%|███████▎  | 22/30 [14:44<05:17, 39.70s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  77%|███████▋  | 23/30 [15:23<04:37, 39.57s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.42s/it]\u001b[A\r",
      "Epoch:  80%|████████  | 24/30 [16:03<03:56, 39.49s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A09/19/2019 14:57:56 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-400\n",
      "\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.53s/it]\u001b[A\r",
      "Epoch:  83%|████████▎ | 25/30 [16:42<03:17, 39.56s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  87%|████████▋ | 26/30 [17:22<02:37, 39.48s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.46s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  90%|█████████ | 27/30 [18:01<01:58, 39.43s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.42s/it]\u001b[A\r",
      "Epoch:  93%|█████████▎| 28/30 [18:40<01:18, 39.42s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A09/19/2019 14:59:59 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-450\n",
      "\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:05<00:36,  2.60s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:33,  2.55s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:10<00:30,  2.53s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.51s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:15<00:24,  2.50s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.49s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:20<00:19,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:25<00:14,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:30<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch:  97%|█████████▋| 29/30 [19:20<00:39, 39.50s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.48s/it]\u001b[A\n",
      "\r",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 16/16 [00:39<00:00,  2.41s/it]\u001b[A\r",
      "Epoch: 100%|██████████| 30/30 [19:59<00:00, 39.46s/it]\n",
      "09/19/2019 15:01:13 - INFO - __main__ -    global_step = 480, average loss = 1.4681171834468842\n",
      "09/19/2019 15:01:13 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/config.json\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/pytorch_model.bin\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' is a path or url to a directory containing tokenizer files.\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/vocab.txt\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/added_tokens.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/special_tokens_map.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/tokenizer_config.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' is a path or url to a directory containing tokenizer files.\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/vocab.txt\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/added_tokens.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/special_tokens_map.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/tokenizer_config.json\n",
      "09/19/2019 15:01:21 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/']\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/config.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/pytorch_model.bin\n",
      "09/19/2019 15:01:25 - INFO - __main__ -   Loading features from cached file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns/cached_dev_bert-base-uncased_15_gns\n",
      "09/19/2019 15:01:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "09/19/2019 15:01:25 - INFO - __main__ -     Num examples = 2036\n",
      "09/19/2019 15:01:25 - INFO - __main__ -     Batch size = 512\n",
      "\r",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Evaluating:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]\r",
      "Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.07it/s]\r",
      "Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s]\r",
      "Evaluating: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n",
      "09/19/2019 15:01:28 - INFO - __main__ -   ***** Eval results  *****\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     acc = 0.75\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     acc_and_f1 = 0.75\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     f1 = 0.75\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=512   \\\n",
    "    --per_gpu_train_batch_size=512   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/ 2>&1 | tee run.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Training V2 (bert-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-large-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=128   \\\n",
    "    --per_gpu_train_batch_size=128   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 100.0 \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/bert-large/ 2>&1 | tee run_bert_large_uncased.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-large-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=128   \\\n",
    "    --per_gpu_train_batch_size=128   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 100.0 \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/bert-large/ 2>&1 | tee run_bert_large_uncased.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Training V3 (roberta-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-large \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=92   \\\n",
    "    --per_gpu_train_batch_size=92   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 100.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/roberta-large/ 2>&1 | tee run_roberta_large.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training V4 (xlnet-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type xlnet \\\n",
    "    --model_name_or_path xlnet-large-cased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=92   \\\n",
    "    --per_gpu_train_batch_size=92   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 100.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/xlnet-large-cased/ 2>&1 | tee run_xlnet_large_cased.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
