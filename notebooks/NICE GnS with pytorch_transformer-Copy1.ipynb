{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "Collecting sacremoses (from pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/24/0b86f494d3a5c7531f6d0c77d39fd8f9d42e651244505d3d737e31db9a4d/sacremoses-0.0.33.tar.gz (802kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (4.32.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (2.20.0)\n",
      "Collecting sentencepiece (from pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (2018.1.10)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-transformers) (1.9.213)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (1.11.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (6.7)\n",
      "Collecting joblib (from sacremoses->pytorch-transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.213 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (1.12.213)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.213->boto3->pytorch-transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.213->boto3->pytorch-transformers) (2.7.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses: started\n",
      "  Running setup.py bdist_wheel for sacremoses: finished with status 'done'\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/70/87/56/e40575cca30d12fee8875d523b8878b7aba866a9f03b2fd983\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: joblib, sacremoses, sentencepiece, pytorch-transformers\n",
      "Successfully installed joblib-0.13.2 pytorch-transformers-1.2.0 sacremoses-0.0.33 sentencepiece-0.1.83\n",
      "Collecting tensorboardX (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (3.5.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorboardX->-r requirements.txt (line 1)) (39.1.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\n",
      "Usage:   \n",
      "  pip uninstall [options] <package> ...\n",
      "  pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -U\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pytorch-transformers\n",
    "\n",
    "cd /home/ec2-user/SageMaker/pytorch-transformers/examples\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "#Fixing error with from tensorboardX import SummaryWriter\n",
    "#TypeError: __new__() got an unexpected keyword argument 'serialized_options'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/f4/a27952733796330cd17c17ea1f974459f5fefbbad119c0f296a6d807fec3/protobuf-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Requirement not upgraded as not directly required: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf) (1.11.0)\n",
      "Requirement not upgraded as not directly required: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf) (39.1.0)\n",
      "Installing collected packages: protobuf\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed protobuf-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastai 1.0.55 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate pytorch_p36\n",
    "pip install protobuf -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Generate Data suitable with run_glue example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>combusting preparations [chemical additives to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10002</td>\n",
       "      <td>adhesives for industrial purposes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10003</td>\n",
       "      <td>salt for preserving, other than for foodstuffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10004</td>\n",
       "      <td>auxiliary fluids for use with abrasives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>vulcanization accelerators</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Classification                                                 EN\n",
       "0      1           10001  combusting preparations [chemical additives to...\n",
       "1      1           10002                  adhesives for industrial purposes\n",
       "2      1           10003     salt for preserving, other than for foodstuffs\n",
       "3      1           10004            auxiliary fluids for use with abrasives\n",
       "4      1           10005                         vulcanization accelerators"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'ipa-cognitive-futures'\n",
    "object_name = 'cf_datasets/TM/raw/nc020-a04_ibli.csv'\n",
    "file_name = 'nc020-a04_ibli.csv'\n",
    "semantic_column = 'EN'\n",
    "\n",
    "s3.download_file(bucket_name, object_name, file_name)\n",
    "\n",
    "import pandas as pd\n",
    "nice_df = pd.read_csv(file_name, header=0,encoding='latin1')\n",
    "\n",
    "#correct columns\n",
    "retrieved_columns = ['Class','Classification','EN', 'FR']\n",
    "columns = retrieved_columns + list(nice_df.columns)[4:]\n",
    "\n",
    "#obtain EN GS\n",
    "nice_df.columns = columns\n",
    "nice_df = nice_df[retrieved_columns]\n",
    "del nice_df['FR']\n",
    "nice_df.dropna(inplace=True)\n",
    "nice_df['Class'] = nice_df['Class'].astype(int)\n",
    "nice_df['Classification'] = nice_df['Classification'].astype(int)\n",
    "nice_df.reset_index(inplace=True,drop=True)\n",
    "nice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train, dev = train_test_split(nice_df, train_size=0.8, random_state=7, stratify=list(nice_df['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>752</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>613</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 752  752\n",
       "2                 132  132\n",
       "3                 272  272\n",
       "4                 110  110\n",
       "5                 525  525\n",
       "6                 506  506\n",
       "7                 613  613\n",
       "8                 300  300\n",
       "9                 832  832\n",
       "10                298  298"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice_df.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>405</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>666</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 602  602\n",
       "2                 106  106\n",
       "3                 218  218\n",
       "4                  88   88\n",
       "5                 420  420\n",
       "6                 405  405\n",
       "7                 490  490\n",
       "8                 240  240\n",
       "9                 666  666\n",
       "10                238  238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification   EN\n",
       "Class                     \n",
       "1                 150  150\n",
       "2                  26   26\n",
       "3                  54   54\n",
       "4                  22   22\n",
       "5                 105  105\n",
       "6                 101  101\n",
       "7                 123  123\n",
       "8                  60   60\n",
       "9                 166  166\n",
       "10                 60   60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.groupby('Class').count().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns'\n",
    "train.to_csv(dataset_folder + '/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev.to_csv(dataset_folder + '/dev.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Glue Training on GnS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/19/2019 14:41:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "09/19/2019 14:41:05 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "09/19/2019 14:41:05 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 14:41:06 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ec2-user/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "09/19/2019 14:41:07 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "09/19/2019 14:41:11 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "09/19/2019 14:41:11 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=15, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=30.0, output_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=512, per_gpu_train_batch_size=512, save_steps=50, seed=42, server_ip='', server_port='', task_name='gns', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   Loading features from cached file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns/cached_train_bert-base-uncased_15_gns\n",
      "09/19/2019 14:41:13 - INFO - __main__ -   ***** Running training *****\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Num examples = 8144\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Num Epochs = 30\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Instantaneous batch size per GPU = 512\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "09/19/2019 14:41:13 - INFO - __main__ -     Total optimization steps = 480\n",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:38,  2.56s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:35,  2.52s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:26,  2.45s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.44s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:21,  2.43s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.43s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:21<00:17,  2.43s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.43s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:26<00:12,  2.43s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.43s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:31<00:07,  2.43s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.43s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.43s/it]\u001b[A\n",
      "Epoch:   3%|▎         | 1/30 [00:38<18:43, 38.74s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.44s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.44s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.44s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.45s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:26,  2.45s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.45s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.45s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.45s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.45s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.45s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:26<00:12,  2.45s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.45s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:31<00:07,  2.45s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.45s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.45s/it]\u001b[A\n",
      "Epoch:   7%|▋         | 2/30 [01:17<18:07, 38.83s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.46s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.46s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  10%|█         | 3/30 [01:56<17:31, 38.95s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A09/19/2019 14:43:19 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-50\n",
      "\n",
      "Iteration:  12%|█▎        | 2/16 [00:08<00:51,  3.65s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:11<00:43,  3.32s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:13<00:36,  3.06s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:16<00:31,  2.88s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:27,  2.75s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:23,  2.66s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:23<00:20,  2.60s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:17,  2.56s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:15,  2.53s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:12,  2.51s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:09,  2.50s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:35<00:07,  2.48s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:40<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  13%|█▎        | 4/30 [02:40<17:25, 40.21s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  17%|█▋        | 5/30 [03:19<16:38, 39.93s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.48s/it]\u001b[A\n",
      "Epoch:  20%|██        | 6/30 [03:58<15:54, 39.76s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A09/19/2019 14:45:26 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-100\n",
      "\n",
      "Iteration:  25%|██▌       | 4/16 [00:13<00:43,  3.65s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:16<00:36,  3.31s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:30,  3.06s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:25,  2.88s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:23<00:22,  2.75s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:18,  2.67s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:15,  2.61s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:12,  2.57s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:10,  2.54s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.51s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.50s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.49s/it]\u001b[A\n",
      "Epoch:  23%|██▎       | 7/30 [04:42<15:38, 40.81s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  27%|██▋       | 8/30 [05:21<14:47, 40.36s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.50s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.49s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.49s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  30%|███       | 9/30 [06:00<14:01, 40.06s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A09/19/2019 14:47:33 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-150\n",
      "\n",
      "Iteration:  38%|███▊      | 6/16 [00:18<00:36,  3.65s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:21<00:29,  3.31s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:23<00:24,  3.05s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:20,  2.88s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:16,  2.76s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:13,  2.67s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:10,  2.60s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.56s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.53s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:40<00:02,  2.51s/it]\u001b[A\n",
      "Epoch:  33%|███▎      | 10/30 [06:43<13:40, 41.02s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.48s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  37%|███▋      | 11/30 [07:23<12:49, 40.50s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  40%|████      | 12/30 [08:02<12:02, 40.14s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A09/19/2019 14:49:39 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-200\n",
      "\n",
      "Iteration:  50%|█████     | 8/16 [00:23<00:29,  3.64s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:26<00:23,  3.31s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:18,  3.06s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:14,  2.88s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:11,  2.75s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:07,  2.66s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.60s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.56s/it]\u001b[A\n",
      "Epoch:  43%|████▎     | 13/30 [08:45<11:38, 41.08s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.48s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.48s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  47%|████▋     | 14/30 [09:25<10:48, 40.56s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.45s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 15/30 [10:04<10:02, 40.19s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.48s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A09/19/2019 14:51:46 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-250\n",
      "\n",
      "Iteration:  62%|██████▎   | 10/16 [00:28<00:21,  3.65s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:31<00:16,  3.32s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:33<00:12,  3.06s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:36<00:08,  2.88s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:38<00:05,  2.76s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:41<00:02,  2.67s/it]\u001b[A\n",
      "Epoch:  53%|█████▎    | 16/30 [10:47<09:35, 41.14s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  57%|█████▋    | 17/30 [11:27<08:47, 40.60s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 18/30 [12:06<08:02, 40.20s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.48s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.48s/it]\u001b[A09/19/2019 14:53:50 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-300\n",
      "\n",
      "Iteration:  75%|███████▌  | 12/16 [00:30<00:10,  2.60s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.56s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:35<00:05,  2.54s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.52s/it]\u001b[A\n",
      "Epoch:  63%|██████▎   | 19/30 [12:46<07:20, 40.08s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.46s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.46s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.46s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.46s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 20/30 [13:25<06:38, 39.84s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  70%|███████   | 21/30 [14:04<05:57, 39.69s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A09/19/2019 14:55:53 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-350\n",
      "\n",
      "Iteration:  88%|████████▊ | 14/16 [00:35<00:05,  2.60s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.56s/it]\u001b[A\n",
      "Epoch:  73%|███████▎  | 22/30 [14:44<05:17, 39.70s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  77%|███████▋  | 23/30 [15:23<04:37, 39.57s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.46s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 24/30 [16:03<03:56, 39.49s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A09/19/2019 14:57:56 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-400\n",
      "\n",
      "Epoch:  83%|████████▎ | 25/30 [16:42<03:17, 39.56s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:36,  2.46s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.46s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:31,  2.46s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.46s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.46s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.46s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  87%|████████▋ | 26/30 [17:22<02:37, 39.48s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.46s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.46s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  90%|█████████ | 27/30 [18:01<01:58, 39.43s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.47s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.47s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.48s/it]\u001b[A\n",
      "Epoch:  93%|█████████▎| 28/30 [18:40<01:18, 39.42s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.48s/it]\u001b[A09/19/2019 14:59:59 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/checkpoint-450\n",
      "\n",
      "Iteration:  12%|█▎        | 2/16 [00:05<00:36,  2.60s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:33,  2.55s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:10<00:30,  2.53s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.51s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:15<00:24,  2.50s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.49s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:20<00:19,  2.48s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:25<00:14,  2.48s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:30<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch:  97%|█████████▋| 29/30 [19:20<00:39, 39.50s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 1/16 [00:02<00:37,  2.47s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 2/16 [00:04<00:34,  2.47s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 3/16 [00:07<00:32,  2.47s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 4/16 [00:09<00:29,  2.47s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 5/16 [00:12<00:27,  2.47s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 6/16 [00:14<00:24,  2.48s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 7/16 [00:17<00:22,  2.48s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 8/16 [00:19<00:19,  2.47s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 9/16 [00:22<00:17,  2.47s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 11/16 [00:27<00:12,  2.47s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 12/16 [00:29<00:09,  2.47s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 14/16 [00:34<00:04,  2.47s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 15/16 [00:37<00:02,  2.47s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 30/30 [19:59<00:00, 39.46s/it]/it]\u001b[A\n",
      "09/19/2019 15:01:13 - INFO - __main__ -    global_step = 480, average loss = 1.4681171834468842\n",
      "09/19/2019 15:01:13 - INFO - __main__ -   Saving model checkpoint to /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/config.json\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 15:01:17 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/pytorch_model.bin\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' is a path or url to a directory containing tokenizer files.\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/vocab.txt\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/added_tokens.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/special_tokens_map.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/tokenizer_config.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/' is a path or url to a directory containing tokenizer files.\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/vocab.txt\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/added_tokens.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/special_tokens_map.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/tokenizer_config.json\n",
      "09/19/2019 15:01:21 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/']\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/config.json\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/19/2019 15:01:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/pytorch_model.bin\n",
      "09/19/2019 15:01:25 - INFO - __main__ -   Loading features from cached file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns/cached_dev_bert-base-uncased_15_gns\n",
      "09/19/2019 15:01:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "09/19/2019 15:01:25 - INFO - __main__ -     Num examples = 2036\n",
      "09/19/2019 15:01:25 - INFO - __main__ -     Batch size = 512\n",
      "Evaluating: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n",
      "09/19/2019 15:01:28 - INFO - __main__ -   ***** Eval results  *****\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     acc = 0.75\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     acc_and_f1 = 0.75\n",
      "09/19/2019 15:01:28 - INFO - __main__ -     f1 = 0.75\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=512   \\\n",
    "    --per_gpu_train_batch_size=512   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/ 2>&1 | tee run.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Training V2 (bert-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-large-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=128   \\\n",
    "    --per_gpu_train_batch_size=128   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/bert-large/ 2>&1 | tee run_bert_large_uncased.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Training V3 (roberta-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/19/2019 15:42:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "09/19/2019 15:42:08 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json not found in cache or force_download set to True, downloading to /tmp/tmprj3gx03q\n",
      "100%|██████████| 474/474 [00:00<00:00, 488837.00B/s]\n",
      "09/19/2019 15:42:09 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmprj3gx03q to cache at /home/ec2-user/.cache/torch/pytorch_transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.233bd69ec613d2ebcb1d55823dfc5b1e109157918e13bdbde6db7f694e1a0039\n",
      "09/19/2019 15:42:09 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.233bd69ec613d2ebcb1d55823dfc5b1e109157918e13bdbde6db7f694e1a0039\n",
      "09/19/2019 15:42:09 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmprj3gx03q\n",
      "09/19/2019 15:42:09 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.233bd69ec613d2ebcb1d55823dfc5b1e109157918e13bdbde6db7f694e1a0039\n",
      "09/19/2019 15:42:09 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "09/19/2019 15:42:10 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json not found in cache or force_download set to True, downloading to /tmp/tmpqkbu4x5n\n",
      "100%|██████████| 898823/898823 [00:00<00:00, 902068.84B/s]\n",
      "09/19/2019 15:42:12 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpqkbu4x5n to cache at /home/ec2-user/.cache/torch/pytorch_transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "09/19/2019 15:42:12 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "09/19/2019 15:42:12 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpqkbu4x5n\n",
      "09/19/2019 15:42:13 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt not found in cache or force_download set to True, downloading to /tmp/tmp6vtf1ex8\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 575362.83B/s]\n",
      "09/19/2019 15:42:14 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp6vtf1ex8 to cache at /home/ec2-user/.cache/torch/pytorch_transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "09/19/2019 15:42:14 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "09/19/2019 15:42:14 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp6vtf1ex8\n",
      "09/19/2019 15:42:14 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "09/19/2019 15:42:14 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ec2-user/.cache/torch/pytorch_transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "09/19/2019 15:42:15 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpvg91nkx3\n",
      "100%|██████████| 1425941629/1425941629 [03:03<00:00, 7765226.20B/s]\n",
      "09/19/2019 15:45:20 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpvg91nkx3 to cache at /home/ec2-user/.cache/torch/pytorch_transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n",
      "09/19/2019 15:45:21 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n",
      "09/19/2019 15:45:21 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpvg91nkx3\n",
      "09/19/2019 15:45:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/pytorch_transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n",
      "09/19/2019 15:45:34 - INFO - pytorch_transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "09/19/2019 15:45:34 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py\", line 495, in <module>\n",
      "    main()\n",
      "  File \"/home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py\", line 440, in main\n",
      "    model.to(args.device)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 386, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 193, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 193, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 193, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 199, in _apply\n",
      "    param.data = fn(param.data)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 384, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 11.17 GiB total capacity; 230.41 MiB already allocated; 7.69 MiB free; 5.59 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-large \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=64   \\\n",
    "    --per_gpu_train_batch_size=64   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/roberta-large/ 2>&1 | tee run_roberta_large.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training V4 (xlnet-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type xlnet \\\n",
    "    --model_name_or_path xlnet-large-cased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=92   \\\n",
    "    --per_gpu_train_batch_size=92   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/xlnet-large-cased/ 2>&1 | tee run_xlnet_large_cased.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/19/2019 15:18:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-config.json not found in cache or force_download set to True, downloading to /tmp/tmpk8a9erro\n",
      "100%|██████████| 306/306 [00:00<00:00, 245684.73B/s]\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpk8a9erro to cache at /home/ec2-user/.cache/torch/pytorch_transformers/063cbd65bb7d2e7fa034126477f72870c897d51a5e29a6baf2ebe35acf00810c.a9584498ff24d6bef104dcc2693a9efab757d2e5ad782c797c29c89fa445b552\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/063cbd65bb7d2e7fa034126477f72870c897d51a5e29a6baf2ebe35acf00810c.a9584498ff24d6bef104dcc2693a9efab757d2e5ad782c797c29c89fa445b552\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpk8a9erro\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-config.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/063cbd65bb7d2e7fa034126477f72870c897d51a5e29a6baf2ebe35acf00810c.a9584498ff24d6bef104dcc2693a9efab757d2e5ad782c797c29c89fa445b552\n",
      "09/19/2019 15:18:59 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 2048,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"finetuning_task\": \"gns\",\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_index\": 5,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 1,\n",
      "  \"n_layers\": 12,\n",
      "  \"n_words\": 30145,\n",
      "  \"num_labels\": 45,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_index\": 2,\n",
      "  \"pruned_heads\": {},\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"unk_index\": 3,\n",
      "  \"use_lang_emb\": true\n",
      "}\n",
      "\n",
      "09/19/2019 15:19:01 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-vocab.json from cache at /home/ec2-user/.cache/torch/pytorch_transformers/6ce36d71d55c65bfdaee86d8ff7aa5118c45f98954b8893e0741c9ee6c61cd4a.dbe0e43202ae562bef29b0aaf02c97af23e2f0a2eb6100e071337d1f732be242\n",
      "09/19/2019 15:19:01 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-merges.txt from cache at /home/ec2-user/.cache/torch/pytorch_transformers/7a5b97f72ab092e8b0e2f268addeb588e41ca9fc9c78d8e9621de6e87cdfe787.992606ccc2e230e4b29e0a73a65a30dff361d6a3db3406289a6657c704fb0428\n",
      "09/19/2019 15:19:02 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpc9pc5ss9\n",
      "100%|██████████| 2668507970/2668507970 [05:44<00:00, 7739082.04B/s]\n",
      "09/19/2019 15:24:48 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpc9pc5ss9 to cache at /home/ec2-user/.cache/torch/pytorch_transformers/97e0df6ef530dce00403c3aaebafb184f59b0f1ffc49608b4a9b0fd7cfef73d1.644de5a0189ce3e598b322f1b099a50277c73207d778efcbbb44abbd7ca6bc0d\n",
      "09/19/2019 15:24:50 - INFO - pytorch_transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/pytorch_transformers/97e0df6ef530dce00403c3aaebafb184f59b0f1ffc49608b4a9b0fd7cfef73d1.644de5a0189ce3e598b322f1b099a50277c73207d778efcbbb44abbd7ca6bc0d\n",
      "09/19/2019 15:24:50 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpc9pc5ss9\n",
      "09/19/2019 15:24:51 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/pytorch_transformers/97e0df6ef530dce00403c3aaebafb184f59b0f1ffc49608b4a9b0fd7cfef73d1.644de5a0189ce3e598b322f1b099a50277c73207d778efcbbb44abbd7ca6bc0d\n",
      "09/19/2019 15:25:13 - INFO - pytorch_transformers.modeling_utils -   Weights of XLMForSequenceClassification not initialized from pretrained model: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "09/19/2019 15:25:13 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "09/19/2019 15:25:16 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=15, max_steps=-1, model_name_or_path='xlm-mlm-en-2048', model_type='xlm', n_gpu=1, no_cuda=False, num_train_epochs=30.0, output_dir='/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/output/gns/xlm-mlm-en-2048/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=50, seed=42, server_ip='', server_port='', task_name='gns', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "09/19/2019 15:25:16 - INFO - __main__ -   Creating features from dataset file at /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   Writing example 0 of 8144\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   *** Example ***\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   guid: train-0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   tokens: </s> escal ators</w> </s>\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_ids: 1 10111 3408 1 2 2 2 2 2 2 2 2 2 2 2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   label: 7 (id = 6)\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   *** Example ***\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   guid: train-1\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   tokens: </s> must</w> </s>\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_ids: 1 537 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   label: 32 (id = 31)\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   *** Example ***\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   guid: train-2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   tokens: </s> in voic ing</w> machines</w> </s>\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_ids: 1 232 10844 533 5967 1 2 2 2 2 2 2 2 2 2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   label: 9 (id = 8)\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   *** Example ***\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   guid: train-3\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   tokens: </s> ma h- jong</w> </s>\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_ids: 1 496 5517 20272 1 2 2 2 2 2 2 2 2 2 2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   label: 28 (id = 27)\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   *** Example ***\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   guid: train-4\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   tokens: </s> inclined</w> ways</w> for</w> boats</w> </s>\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_ids: 1 13224 1619 29 4311 1 2 2 2 2 2 2 2 2 2\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "09/19/2019 15:25:16 - INFO - utils_glue -   label: 12 (id = 11)\n",
      "09/19/2019 15:25:19 - INFO - __main__ -   Saving features into cached file /home/ec2-user/SageMaker/pytorch-transformers/examples/datasets/gns/cached_train_xlm-mlm-en-2048_15_gns\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\", line 149, in _with_file_like\n",
      "    return body(f)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\", line 224, in <lambda>\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\", line 297, in _save\n",
      "    pickler.dump(obj)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py\", line 495, in <module>\n",
      "    main()\n",
      "  File \"/home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py\", line 447, in main\n",
      "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
      "  File \"/home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py\", line 287, in load_and_cache_examples\n",
      "    torch.save(features, cached_features_file)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\", line 224, in save\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\", line 152, in _with_file_like\n",
      "    f.close()\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tee: run_xlm_mlm_en_2048.log: No space left on device\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "export GLUE_DIR=/home/ec2-user/SageMaker/pytorch-transformers/examples/datasets\n",
    "export TASK_NAME=gns\n",
    "\n",
    "python /home/ec2-user/SageMaker/pytorch-transformers/examples/run_glue.py \\\n",
    "    --model_type xlm \\\n",
    "    --model_name_or_path xlm-mlm-en-2048 \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 15 \\\n",
    "    --per_gpu_eval_batch_size=64   \\\n",
    "    --per_gpu_train_batch_size=64   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir $GLUE_DIR/output/$TASK_NAME/xlm-mlm-en-2048/ 2>&1 | tee run_xlm_mlm_en_2048.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
